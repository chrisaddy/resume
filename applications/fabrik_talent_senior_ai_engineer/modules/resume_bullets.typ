// Bullet points for work experience (customizable per job)

// URBN
#let urbn_bullets = (
  "Led migration from Airflow to Flyte, engineered high-performance ML pipelines with focus on inference scalability and throughput optimization.",
  "Redesigned MLflow architecture to improve model reproducibility and implemented benchmarking systems for ML performance metrics across distributed infrastructure.",
  "Optimized PyTorch inference pipelines for production deployment, reducing latency by 40% and improving GPU utilization efficiency.",
  "Architected high-throughput data systems using DuckDB that reduced processing time by 60%, enabling real-time model inference and feedback loops.",
  "Collaborated with cross-functional teams to implement performance-focused MLOps practices, reducing model deployment time from days to hours.",
)

// Pepsico Head ML Engineering
#let pepsico_head_bullets = (
  "Led distributed inference system design for LLMs, optimizing performance for low-latency serving across GPU clusters that reduced model response times by 65%.",
  "Designed high-throughput C++ and PyTorch inference architecture for real-time recommendation models that scaled to handle 200k+ requests per minute.",
  "Engineered model optimization pipeline including quantization techniques that improved inference GPU utilization by 40% while maintaining model accuracy.",
  "Founded the Generative AI Lab, building a team focused on performance optimization for LLM deployment, including CUDA-accelerated text generation systems.",
  "Architected profiling and benchmarking infrastructure for ML pipeline optimization, identifying and resolving performance bottlenecks that decreased model deployment time by 300%.",
)

// Pepsico Principal Data Scientist
#let pepsico_principal_bullets = (
  "Led performance engineering for ML inference systems, reducing model serving latency by 70% through advanced GPU optimization techniques and parallel processing.",
  "Architected distributed ML pipeline infrastructure via KubeFlow with C++ optimization layers, resulting in 50% faster model deployment to production.",
  "Implemented low-latency system architecture for model serving, enabling real-time inference capabilities with optimized throughput for high-traffic applications.",
  "Integrated PyTorch JIT compilation and TensorRT optimizations for predictive ML models, resulting in 3x performance improvement for inference workloads.",
  "Engineered comprehensive benchmarking infrastructure that systematically identified performance bottlenecks, cutting costs by over $10k per month.",
  "Optimized AWS GPU instance utilization, implementing model parallelism strategies that reduced inference times by 50% while improving scalability.",
  "Developed high-performance data pipelines with distributed processing capabilities, standardizing ingestion and transformation processes for ML model inference.",
  "Led cross-team collaboration to productionize inference pipelines with focus on throughput optimization, enabling 7 concurrent model deployments with minimal latency overhead.",
)

// Penn Interactive
#let penn_interactive_bullets = (
  "Founded the ML engineering function with focus on high-performance, low-latency inference systems for real-time sports betting applications processing 20k+ concurrent requests.",
  "Implemented GPU-accelerated prediction models with C++ optimization layers that reduced inference latency from seconds to milliseconds, enabling real-time betting features.",
  "Engineered high-throughput ML algorithm delivery infrastructure with optimized PyTorch serving, significantly reducing model loading times and improving system responsiveness.",
  "Architected distributed inference system for BERT-based language models with custom TensorRT optimizations, decreasing processing time by 80% while maintaining accuracy.",
  "Designed comprehensive performance benchmarking framework for ML systems, enabling data-backed optimization decisions that improved overall platform responsiveness by 50%.",
  "Implemented real-time monitoring and profiling tools for ML inference systems, creating dashboards that identified performance bottlenecks across the distributed architecture.",
)

// Heavywater
#let heavywater_bullets = (
  "Optimized model inference pipeline for financial document classification by upgrading a legacy system to a high-throughput distributed architecture, reducing processing time by 70% for 1200+ page mortgage packets.",
  "Implemented GPU acceleration and parallel processing for document processing models, achieving 96% accuracy while increasing inference speeds from 3-4 to 30-50 documents per hour through optimized execution strategies.",
  "Engineered performance-focused NLP pipelines with PyTorch for text extraction, improving model inference latency by 60% and developing benchmarking systems to continuously optimize algorithm performance.",
)

// Education bullets
#let temple_bullets = (
  "Relevant coursework in statistical machine learning, survival analysis, structural equation modeling, and multivariate time-series modeling.",
)

#let lehigh_bullets = (
  "Relevant coursework in time-series forecasting and causal modeling.",
)

// Project bullets
#let pocketsize_bullets = (
  "Co-founder and CTO of Pocket Size Fund, an open source quantitative hedge fund.",
)