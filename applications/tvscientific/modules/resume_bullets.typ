// Bullet points for work experience (customized for tvScientific)

// URBN
#let urbn_bullets = (
  "Engineered high-performance ML systems by migrating from Airflow to Flyte, significantly improving pipeline reliability and reducing inference latency by 40% for production ML workflows.",
  "Optimized ML model serving infrastructure to support low-latency, high-throughput requirements, handling 100+ requests per second while maintaining sub-50ms response times.",
  "Implemented robust ML deployment workflows connecting Python development environment with production systems, enabling seamless model transitions from research to deployment.",
  "Improved system performance by replacing pandas with DuckDB for ETL operations, achieving 3x faster data processing with 50% lower resource consumption for critical ML pipelines.",
  "Established cross-functional collaboration practices between data scientists and engineers, developing standardized interfaces for ML system integration and technical documentation.",
)

// Pepsico Head ML Engineering
#let pepsico_head_bullets = (
  "Architected high-performance ML systems that reduced inference pipeline latency by 70%, enabling real-time decision making for customer-facing recommendation engines.",
  "Led system optimization efforts that increased throughput of ML services by 200%, allowing scaled processing of millions of customer interactions daily.",
  "Developed Python-based ML infrastructure that bridged data science research with production systems, decreasing model deployment time from months to days.",
  "Established development best practices for ML systems engineering, implementing comprehensive testing, monitoring, and documentation standards across the organization.",
  "Directed cross-functional teams of ML engineers and data scientists, creating a collaborative environment that accelerated innovation and reduced technical debt.",
)

// Pepsico Principal Data Scientist
#let pepsico_principal_bullets = (
  "Engineered scalable ML infrastructure via KubeFlow to handle high-throughput workloads, processing thousands of concurrent requests while maintaining consistent performance.",
  "Optimized ML model serving architecture, reducing inference latency by 40% and scaling to support 10x increase in traffic without degradation.",
  "Developed technical documentation and communication practices for complex ML systems, ensuring clear understanding of architecture decisions and performance tradeoffs.",
  "Built Python-based ML pipelines for data ingestion, transformation, and model training that improved development efficiency by 50% across data science teams.",
  "Implemented system performance monitoring and optimization for AWS-based ML infrastructure, reducing costs by over $10k per month while improving reliability.",
  "Right-sized computational resources for ML workloads, balancing performance and cost efficiency while supporting increasing model complexity and data volumes.",
  "Standardized ML system architecture across teams, creating reusable components that accelerated development and ensured consistent performance across applications.",
  "Collaborated with cross-functional teams to design and implement marketing attribution ML systems that operated in near real-time, handling complex data streams efficiently.",
)

// Penn Interactive
#let penn_interactive_bullets = (
  "Designed and implemented real-time, low-latency ML systems for sports betting applications, handling millions of transactions with 99.9% uptime during peak load periods.",
  "Built high-performance infrastructure for ML model deployment, optimizing for throughput and response time in a high-stakes betting environment requiring millisecond-level decisions.",
  "Developed RESTful services for ML model serving that balanced computational efficiency with availability, creating a resilient system that scaled to 75k+ users.",
  "Implemented advanced ML pipeline architecture integrating BERT-based language models with recommendation systems, orchestrating complex data flows with minimal latency.",
  "Established performance testing and optimization practices for ML systems, identifying and resolving bottlenecks to improve overall system throughput by 200%.",
  "Created real-time monitoring dashboards that tracked ML system performance, enabling proactive optimization and rapid response to changing traffic patterns.",
)

// Heavywater
#let heavywater_bullets = (
  "Optimized document classification system performance, improving throughput from 4 to 50 documents per hour while maintaining 96% accuracy on complex 1200+ page financial documents.",
  "Re-engineered legacy ML systems into a unified architecture, significantly reducing latency and computational resource requirements while expanding processing capabilities.",
  "Developed performance-focused data extraction pipelines that balanced accuracy with processing speed, achieving optimal throughput for critical financial document processing.",
)

// Education bullets
#let temple_bullets = (
  "Relevant coursework in computational systems, high-performance computing, statistical machine learning, and algorithm optimization for large-scale data processing.",
)

#let lehigh_bullets = (
  "Coursework in computational methods, system design, and optimization techniques for data-intensive applications.",
)

// Project bullets
#let pocketsize_bullets = (
  "Engineered high-performance ML trading systems requiring microsecond-level latency and throughput for time-sensitive financial decisions, implementing optimized Python-C++ interfaces.",
)